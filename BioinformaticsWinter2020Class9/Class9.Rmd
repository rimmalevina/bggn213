---
title: "Class9: Machine Learning Project"
author: "Rimma Levina"
date: "2/7/2020"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Get our input data

Our data today come from UWisco Medical Center, a .csv file with breast cancer biospy data

```{r}
wisc.df <- read.csv("WisconsinCancer.csv")
head(wisc.df) # gives you a little peak, you notice a problem right away: the last column is a mistake, its named X but it is the result of the researchers accidentally adding a blank column, not realizing, and publishing it like this, making the data very misleading. Always CHECK YOUR DATA.
# we wont include the ID, diagnosis, and the last X column, lets clean it up: select only the columns you want
```


```{r}
wisc.data <- as.matrix(wisc.df[3:32])
head(wisc.data)
```


> Q. How many observations (i.e. patients) are in this dataset?

```{r}
nrow(wisc.data)
```

> Q. How many cancer and non-cancer patients are there in the data? 

```{r}
table(wisc.df$diagnosis) #gives you sum of all Ms and Bs in the diagnosis column
```

> Q. How many variables/features in the data are suffixed with _mean?

```{r}
grep("_mean", colnames(wisc.df))   # this function looks for patterns in your argument within each element of a character vector
# can include value = TRUE to see the names of the columns w your pattern
# you see it gives you the position of the columns with "_mean"
```

Now to see how many columns there actually are use length()

```{r}
length(grep("_mean", colnames(wisc.df)))
```


## Performing PCA on the data

First we need to check whether the data need to be scaled. -> look at SD and mean for the columns

Remember we are working with a data frame, not a vector, so you have to use the apply() function to apply the sd and mean functions across either all rows or columns/whatever you want


```{r}
round(apply(wisc.data, 2, sd)) # margin- 1= rows and 2=columns 
# round function cleans up the numbers
```

So yes, we do have to scale

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

> Q7. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44%

> Q8. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3

> Q9. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7

```{r}
biplot(wisc.pr) # you quickly see that we have too many data points to use biplot to visualize this data
```

This is a hot mess! We need to cook our own PCA plot. To do this we need to access the results within the `wisc.pr`


```{r}
attributes(wisc.pr)
```


"x" is our main result. We want the $x component to make our PCA plot

```{r}
plot(wisc.pr$x[,1:2], col=wisc.df$diagnosis) # to access the first and second PC
```


Each point here is a patient, with all the pieces of data that came along with them. All the information per patient was compressed down into a single point that summarizes a whole patient in a single point.

At its core, PCA compresses a data set with lots of dimensions such that it still represents the essence of the original data by focusing on aspects of the data that are different. 

Lecture slides have a great description of how PCA does this.

So lets start by looking at the variance between the Principle Components - with the goal being that we will crete scree plots that show the proportion of variance explained as the number of principal components increases

????

```{r}
pr.var <- (wisc.pr$sdev)^2
head(pr.var) # gives you first 6 values
pr.var
```

```{r}
pve <- pr.var/wisc.pr$prcomp #Variance explained by each principal component: pve 
pve
```

????


## Hierarchical clustering 
Can help us see what kind of groups naturally occur in the data

As part of the preparation for hierarchical clustering, the distance between all pairs of observations are computed. Furthermore, there are different ways to link clusters together, with single, complete, and average being the most common linkage methods.

Can we find a separation of cancer from non-cancer using a clustering method on the original input data?

For this we will use the `hclust()` function on the `wisc.data` object that we used for PCA.



```{r}
hc <- hclust( dist(wisc.data) )
plot(hc)
```

I don't know where it is good to *cut* a tree like this...

I can cluster in PC space - in other words use the results of PCA to do my clustering

```{r}
wisc.pr.hc <- hclust( dist(wisc.pr$x[,1:3]), method="ward.D2" )
plot(wisc.pr.hc)
```

```{r}
grps <- cutree(wisc.pr.hc, k=2)
table(grps)
```

```{r}
table(grps, wisc.df$diagnosis) #we can determine the false/true postivies and negatives
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```


## Prediction using out PCA model

We will use the `predict()` function that will taje out PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=wisc.df$diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], labels=c(1,2), col="white")
```












